import os
import logging
import asyncio
import base64
import audioop
import wave
import json
from io import BytesIO
from collections import deque
from dataclasses import dataclass
from datetime import datetime
from quart import Quart, request, Response, websocket
from dotenv import load_dotenv
from asyncio import Queue
import uuid

# Global Variables and Events
call_ended = asyncio.Event()
ready_chunks: Queue = Queue() # Queue of AudioChunks
transcription_queue : Queue = Queue() # Another Queue of AudioChunks

# Load environment variables
load_dotenv()

# Configure logging
logger = logging.getLogger("twilio_handler")
logger.setLevel(logging.INFO if os.getenv("LOGGING_ENABLED", "false").lower() == "true" else logging.CRITICAL)

# Create Quart app
app = Quart(__name__)

# --- Data Structures ---
@dataclass
class AudioChunk:
    phrase_id: str
    chunk_index: int
    audio_bytes: bytes
    rms: float
    timestamp: float  # seconds since start of stream
    duration: float # should become len(audio_chunk.audio_bytes) / (8000 * 2)
    transcription: str = ""
    capture_state: str = "" # "listening or "speaking"

async def write_all_chunks_to_disk(queue: asyncio.Queue, output_dir: str):
    os.makedirs(output_dir, exist_ok=True)
    idx = 0
    # Drain the queue completely
    while not queue.empty():
        chunk = await queue.get()
        out_path = os.path.join(output_dir, f"{chunk.phrase_id}__chunk_{chunk.chunk_index}.wav")
        with wave.open(out_path, "wb") as wav_file:
            wav_file.setnchannels(1)
            wav_file.setsampwidth(2)
            wav_file.setframerate(8000)
            wav_file.writeframes(chunk.audio_bytes)
        logger.info(f"Wrote chunk to {out_path}")
        idx += 1
    logger.info(f"Finished writing {idx} chunks to disk.")

# --- Webhook Route ---
@app.route("/voice", methods=["POST"])
async def voice_webhook():
    logger.info("Received /voice webhook from Twilio.")
    twiml = """<?xml version=\"1.0\" encoding=\"UTF-8\"?>
<Response>
  <Connect>
    <Stream url=\"wss://youngbullserver.com/ai_caller/stream\" />
  </Connect>
</Response>"""
    return Response(twiml, mimetype="text/xml")

# --- Audio Utilities ---
def ulaw_to_pcm(ulaw_bytes: bytes) -> bytes:
    return audioop.ulaw2lin(ulaw_bytes, 2)

def calculate_rms(chunk: bytes) -> float:
    try:
        return audioop.rms(audioop.ulaw2lin(chunk, 2), 2)
    except Exception as e:
        logger.error(f"RMS calculation error: {e}")
        return 0.0

# --- WebSocket Route ---
@app.websocket("/stream")
async def media_stream():
    logger.info("WebSocket connection established with Twilio.")
    # Parameters
    MIN_SPEECH_RMS_THRESHOLD = 500.0
    CHUNK_SILENCE_DURATION_SECONDS = 0.5
    DONE_SPEAKING_SILENCE_DURATION_SECONDS = 1.5
    MINCHUNK_DURATION_SECONDS = 0.3
    MAXCHUNK_DURATION_SECONDS = 10.0
    LEAD_IN_DURATION_SECONDS = 0.15

    # Derived
    CHUNK_DURATION_SECONDS = 0.02
    silence_chunk_limit = int(CHUNK_SILENCE_DURATION_SECONDS / CHUNK_DURATION_SECONDS)
    done_speaking_chunk_limit = int(DONE_SPEAKING_SILENCE_DURATION_SECONDS / CHUNK_DURATION_SECONDS)
    min_chunk_chunks = int(MINCHUNK_DURATION_SECONDS / CHUNK_DURATION_SECONDS)
    max_chunk_chunks = int(MAXCHUNK_DURATION_SECONDS / CHUNK_DURATION_SECONDS)
    lead_in_bytes = int(8000 * 2 * LEAD_IN_DURATION_SECONDS)

    phrase_id = str(uuid.uuid4())
    chunk_index = 0
    active_pcm = bytearray()
    pre_chunk_pcm_buffer = deque(maxlen=lead_in_bytes)
    silence_counter = 0
    phrase_silence_counter = 0
    in_chunk = False

    try:
        while True:
            msg = await websocket.receive()
            if msg is None:
                logger.warning("WebSocket closed by Twilio.")
                break

            event = json.loads(msg)
            event_type = event.get("event")

            if event_type == "start":
                logger.info("Twilio media stream started.")

            elif event_type == "stop":
                logger.info("Twilio media stream stopped.")
                call_ended.set()
                break

            elif event_type == "media":
                payload = event["media"].get("payload")
                if not payload:
                    continue

                chunk = base64.b64decode(payload)
                rms = calculate_rms(chunk)
                pcm = ulaw_to_pcm(chunk)
                logger.info(f"Frame #{chunk_index}: RMS={rms:.2f}")

                # Always buffer to pre-chunk ring buffer
                pre_chunk_pcm_buffer.extend(pcm)

                if not in_chunk and rms >= MIN_SPEECH_RMS_THRESHOLD:
                    if phrase_silence_counter >= done_speaking_chunk_limit:
                        logger.info(f"Detected end of phrase. Generating new phrase ID.")
                        phrase_id = str(uuid.uuid4())

                    in_chunk = True
                    silence_counter = 0
                    phrase_silence_counter = 0
                    active_pcm.extend(pre_chunk_pcm_buffer) # Adds a bit of lead in to stop clipping edges to imrpove donwstream STT.
                    pre_chunk_pcm_buffer.clear()
                    logger.info(f"Started new chunk #{chunk_index} for phrase_id={phrase_id}")

                if in_chunk:
                    active_pcm.extend(pcm)

                    if rms < MIN_SPEECH_RMS_THRESHOLD:
                        silence_counter += 1
                        phrase_silence_counter += 1
                    else:
                        silence_counter = 0
                        phrase_silence_counter = 0

                    if silence_counter >= silence_chunk_limit:
                        if len(active_pcm) >= int(MINCHUNK_DURATION_SECONDS * 8000 * 2):
                            logger.info("Attempting to put chunk into ready_chunks and transcription_queue...")

                            chunk = AudioChunk(
                                phrase_id=phrase_id,
                                chunk_index=chunk_index,
                                audio_bytes=bytes(active_pcm),
                                rms=rms,
                                timestamp=chunk_index * CHUNK_DURATION_SECONDS,
                                duration=len(active_pcm) / (8000 * 2),
                                transcription="",
                                capture_state="listening"
                            )

                            await ready_chunks.put(chunk)
                            await transcription_queue.put(chunk)

                            logger.info(f"Successfully added chunk #{chunk_index} with phrase_id={phrase_id} to both queues.")
                            
                            # Reset
                            active_pcm = bytearray()
                            silence_counter = 0
                            in_chunk = False

                # Check for end of phrase outside of speech
                elif rms < MIN_SPEECH_RMS_THRESHOLD:
                    phrase_silence_counter += 1
                    if phrase_silence_counter == done_speaking_chunk_limit:
                        logger.info("Detected end of phrase based on silence threshold.")

                chunk_index += 1

    except Exception as e:
        logger.error(f"WebSocket error: {e}")